# 大模型本质与部署

## 一、LLM 的本质

### 1.1 一句话定义
**大语言模型（LLM）本质上是一个在给定上下文条件下预测"下一个 token"概率分布的函数**。

用数学表示：\(P(token_{t+1} \mid token_1, token_2, ..., token_t)\)

### 1.2 模型 ≠ 数据集
一个常见误解是把"模型"等同于"数据集"。实际上：
- **训练数据**：用于训练的原始语料（训练完成后不再需要）
- **模型**：训练完成后得到的**参数/权重（weights）**，是对训练数据中模式的"压缩表示"


### 1.3 可运行的"模型资产"包含什么
仅有权重文件还不够，完整的模型资产通常包括：

| 组成部分 | 说明 | 示例文件 |
|---------|------|---------|
| 模型架构配置 | 层数、隐藏层维度、注意力头数等 | `config.json` |
| 权重参数 | 训练得到的数值参数 | `model.safetensors`、`pytorch_model.bin` |
| Tokenizer | 文本与 token 之间的映射规则 | `tokenizer.json`、`vocab.txt` |
| 对话模板（可选） | 多轮对话的格式约定 | `chat_template` |

简而言之，大模型 = Transformer 架构 + 训练得到的权重参数 + tokenizer/配置；它在训练阶段用训练框架通过海量数据优化权重，在部署阶段由推理框架加载这些权重并在 GPU 上做前向计算（推理），从而把用户输入生成模型输出。

## 二、训练 vs 推理

这是两个完全不同的阶段，使用不同的工具链：

| 阶段 | 目的 | 权重状态 | 常用框架 |
|-----|------|---------|---------|
| **训练（Training）** | 从数据中学习，优化权重 | 不断更新 | PyTorch、DeepSpeed、Megatron-LM |
| **评估（Evaluation）** | 测试模型能力，发现问题 | 固定 | lm-eval-harness、自定义评测集 |
| **推理（Inference）** | 上线服务，响应用户请求 | 固定 | vLLM、TGI、TensorRT-LLM、llama.cpp |

> **关键区别**：训练需要计算梯度并更新权重（反向传播）；推理只做前向计算，权重不变。

## 三、部署准备清单

### 3.1 模型资产（Model Artifacts）
- **权重文件**：HuggingFace 格式（`safetensors`/`bin`）、量化格式（GGUF/AWQ/GPTQ）、或推理引擎格式（TensorRT engine）
- **配置文件**：`config.json`、`tokenizer.json`、`tokenizer_config.json` 等

### 3.2 推理服务（Runtime）
- **推理框架**：负责加载模型、调度请求、批处理、流式输出
- **API 接口**：HTTP/gRPC，通常兼容 OpenAI API 格式

### 3.3 硬件与驱动（以 NVIDIA GPU 为例）
- **GPU 驱动**：与 GPU 型号匹配
- **CUDA**：深度学习计算的基础运行时
- **可选组件**：cuDNN（深度学习加速）、NCCL（多卡通信）、TensorRT（推理优化）

### 3.4 生产治理
- **服务治理**：鉴权、限流、熔断、灰度发布
- **可观测性**：QPS、延迟（TTFT/TPS）、显存水位、错误率

## 四、部署流程（6 步）

```
1. 选择部署形态
   └─ 单机单卡 / 单机多卡 / 多机多卡 / CPU推理
   
2. 准备 GPU 环境
   └─ 安装驱动 → 验证 nvidia-smi → 确认 CUDA 版本
   
3. 获取模型资产
   └─ 下载权重 + 配置 + tokenizer（版本需匹配）
   
4. 配置推理框架
   └─ 设置：上下文长度、批处理策略、量化精度、并行策略
   
5. 启动服务
   └─ 启动进程 → 健康检查 → 验证请求
   
6. 接入治理
   └─ 网关、鉴权、监控、告警
```

## 五、启动过程详解

"启动大模型"本质是启动一个推理服务进程，主要经历以下阶段：

```
┌─────────────────────────────────────────────────────────┐
│  1. 加载配置与 Tokenizer（CPU 内存）                      │
│     ↓                                                   │
│  2. 读取权重文件（磁盘 → CPU 内存）                       │
│     ↓                                                   │
│  3. 传输权重到 GPU（CPU 内存 → GPU 显存）                 │
│     ↓                                                   │
│  4. 初始化计算内核（编译/加载优化算子）                    │
│     ↓                                                   │
│  5. 预分配 KV Cache 池（显存中预留空间）                   │
│     ↓                                                   │
│  6. 监听端口，进入就绪状态                                │
└─────────────────────────────────────────────────────────┘
```

**观测现象**：启动过程中显存会快速上升，最终稳定在"权重 + KV Cache 预留 + 工作区"的总和。

## 六、请求处理全流程（CPU/GPU 协作）

一次用户请求从输入到输出，经历 **Prefill（预填充）** 和 **Decode（解码）** 两个阶段：

### 6.1 整体流程图

```
用户请求
    │
    ▼
┌───────────────────────────────────────────────────────────┐
│ A. 预处理（CPU）                                           │
│    1. 接收 HTTP/gRPC 请求                                  │
│    2. 拼接上下文（系统提示 + 历史对话 + 用户输入）            │
│    3. Tokenize：文本 → token IDs                          │
│    4. 动态批处理：合并多个请求提升效率                       │
└───────────────────────────────────────────────────────────┘
    │
    ▼
┌───────────────────────────────────────────────────────────┐
│ B. Prefill 阶段（GPU）—— 处理整个 prompt                    │
│    5. 将所有 prompt tokens 送入模型                        │
│    6. 逐层计算 Attention 和 MLP                           │
│    7. 生成 KV Cache 并存入显存                             │
│    8. 输出第一个 token 的概率分布                          │
└───────────────────────────────────────────────────────────┘
    │
    ▼
┌───────────────────────────────────────────────────────────┐
│ C. Decode 阶段（GPU）—— 逐个生成后续 token                  │
│    9. 每步只输入 1 个新 token                              │
│   10. 利用 KV Cache 做增量 Attention（无需重算历史）        │
│   11. 采样得到下一个 token                                 │
│   12. 重复直到遇到结束符或达到长度上限                       │
└───────────────────────────────────────────────────────────┘
    │
    ▼
┌───────────────────────────────────────────────────────────┐
│ D. 后处理（CPU）                                           │
│   13. Detokenize：token IDs → 文本                        │
│   14. 流式返回（SSE/WebSocket）或一次性返回                 │
└───────────────────────────────────────────────────────────┘
    │
    ▼
用户收到回复
```

### 6.2 两个关键性能指标

| 指标 | 含义 | 主要影响因素 |
|-----|------|-------------|
| **TTFT**（Time To First Token） | 用户发出请求到收到第一个字的时间 | Tokenize、排队、Prefill 计算量 |
| **TPS**（Tokens Per Second） | 每秒生成的 token 数 | Decode 效率、KV Cache、显存带宽 |

## 七、大模型与 GPU 的交互机制

### 7.1 数据传输（Host ↔ Device）

| 数据类型 | 传输方向 | 发生时机 |
|---------|---------|---------|
| 模型权重 | 磁盘 → CPU → GPU | 服务启动时 |
| 输入张量（token IDs、mask） | CPU → GPU | 每次请求 |
| 输出（logits、采样结果） | GPU → CPU | 每次生成 |

### 7.2 GPU 上的主要计算

Transformer 推理的计算主要包括：
- **矩阵乘法（GEMM/MatMul）**：占计算量的绝大部分
- **注意力计算（Attention）**：通常使用 FlashAttention 等优化实现
- **归一化与激活**：LayerNorm、RMSNorm、Softmax、SiLU/GELU 等

### 7.3 显存占用构成

```
显存总占用 = 模型权重 + KV Cache + 工作区

┌─────────────────────────────────────────────────────────┐
│  模型权重（Weights）                                      │
│  - 大小固定，与模型参数量和精度相关                         │
│  - 7B FP16 ≈ 14GB，7B INT4 ≈ 3.5GB                       │
├─────────────────────────────────────────────────────────┤
│  KV Cache                                               │
│  - 大小 = 层数 × 2 × hidden_size × 上下文长度 × 并发数     │
│  - 是长上下文和高并发的主要瓶颈                            │
├─────────────────────────────────────────────────────────┤
│  工作区（Workspace）                                      │
│  - 算子计算时的临时缓冲区                                  │
│  - 通常相对较小                                           │
└─────────────────────────────────────────────────────────┘
```

### 7.4 多卡场景

当单卡显存不足时，需要多卡并行：
- **张量并行（Tensor Parallel）**：将每层的权重切分到多卡，需要 NCCL 通信
- **流水并行（Pipeline Parallel）**：不同层放在不同卡上，按流水线方式计算

## 八、部署选型指南

| 场景 | 推荐方案 | 特点 |
|-----|---------|------|
| 快速上线 + 高吞吐 | vLLM、TGI | 动态批处理成熟，KV Cache 管理完善，易于部署 |
| 极致性能 + 低延迟 | TensorRT-LLM | 性能最优，但需要编译引擎，工程投入较高 |
| 本地/边缘/低成本 | llama.cpp + 量化 | 支持 CPU 推理，资源占用低，吞吐有限 |

## 九、常见误区

| 误区 | 正确理解 |
|-----|---------|
| "模型会记住我们的对话" | 模型本身无状态，"记忆"是通过把历史对话拼入上下文实现的 |
| "模型在思考" | 模型只是在做概率计算，逐 token 生成最可能的后续内容 |
| "推理框架用来训练模型" | 推理框架只负责加载已训练好的权重并提供服务，训练用训练框架 |
| "显存越大越好" | 显存需匹配模型大小和并发需求，过大造成浪费 |

## 十、核心要点速记

- **本质**：LLM 是"下一 token 概率预测器"，通过 Transformer 前向计算在上下文条件下生成文本
- **训练 vs 推理**：训练框架把数据变成权重；推理框架把权重变成在线服务
- **交互链路**：用户输入 → Tokenize（CPU）→ Prefill（GPU）→ Decode（GPU）→ Detokenize（CPU）→ 返回用户
- **显存三大块**：权重 + KV Cache + 工作区
