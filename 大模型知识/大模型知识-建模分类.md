# 大模型建模分类

## 一、按语言建模目标分类

这是最核心的分类维度，决定了模型"如何学习语言"。

### 1.1 自回归模型（Autoregressive, AR）

**核心思想**：从左到右，逐个预测下一个 token。

**训练目标**：
\[P(x_1, x_2, ..., x_n) = \prod_{t=1}^{n} P(x_t \mid x_1, x_2, ..., x_{t-1})\]

**特点**：
- 只能看到"过去"的 token，不能看到"未来"
- 天然适合生成任务（写作、对话、代码）
- 推理时逐 token 生成，可流式输出

**架构**：Decoder-only Transformer

**代表模型**：
| 模型 | 开发者 | 参数量 |
|-----|-------|-------|
| GPT-4 / GPT-4o | OpenAI | 未公开（推测万亿级） |
| Claude 3.5 | Anthropic | 未公开 |
| LLaMA 3 | Meta | 8B / 70B / 405B |
| Qwen 2.5 | 阿里 | 0.5B - 72B |
| DeepSeek-V3 | DeepSeek | 671B（MoE） |

> **当前主流**：几乎所有对话/生成类大模型都是自回归架构。

**主流框架与实现**：

| 框架/库 | 类型 | 说明 |
|--------|------|------|
| **HuggingFace Transformers** | 训练/推理 | 最通用的模型库，支持几乎所有开源 AR 模型 |
| **PyTorch + DeepSpeed** | 训练 | 大规模分布式训练标配，ZeRO 显存优化 |
| **Megatron-LM** | 训练 | NVIDIA 出品，张量/流水并行，超大模型预训练 |
| **vLLM** | 推理 | PagedAttention，高吞吐推理服务 |
| **TensorRT-LLM** | 推理 | NVIDIA 极致优化，低延迟 |
| **llama.cpp** | 推理 | C++ 实现，CPU/边缘友好，量化支持 |

**开源实现参考**：
- **LLaMA 系列**：[meta-llama/llama](https://github.com/meta-llama/llama)
- **Qwen 系列**：[QwenLM/Qwen](https://github.com/QwenLM/Qwen)
- **GPT-2/GPT-NeoX**：[EleutherAI/gpt-neox](https://github.com/EleutherAI/gpt-neox)

**典型训练代码结构**（伪代码）：
```python
# 自回归训练核心：预测下一个 token
for batch in dataloader:
    input_ids = batch["input_ids"][:, :-1]   # 输入：去掉最后一个 token
    labels = batch["input_ids"][:, 1:]       # 标签：去掉第一个 token
    
    logits = model(input_ids)                # 前向传播
    loss = cross_entropy(logits, labels)     # 计算交叉熵损失
    loss.backward()                          # 反向传播
    optimizer.step()
```

---

### 1.2 自编码模型（Autoencoding, AE / Masked LM）

**核心思想**：随机遮盖部分 token，让模型预测被遮盖的内容。

**训练目标**（以 BERT 的 MLM 为例）：
- 随机遮盖 15% 的 token
- 模型根据上下文（双向）预测被遮盖的 token

**特点**：
- 可以同时看到"过去"和"未来"（双向注意力）
- 擅长理解任务（分类、NER、相似度计算）
- 不擅长生成（因为训练时不是逐 token 生成）

**架构**：Encoder-only Transformer

**代表模型**：
| 模型 | 开发者 | 说明 |
|-----|-------|------|
| BERT | Google | 开创性工作，NLU 基准 |
| RoBERTa | Meta | 更大数据、更长训练的 BERT |
| ALBERT | Google | 参数共享，更轻量 |
| DeBERTa | Microsoft | 解耦注意力，性能更强 |

> **适用场景**：文本分类、命名实体识别（NER）、句子相似度、信息检索中的语义编码。

**主流框架与实现**：

| 框架/库 | 类型 | 说明 |
|--------|------|------|
| **HuggingFace Transformers** | 训练/推理 | 提供 `BertModel`、`RobertaModel` 等预训练模型 |
| **Sentence-Transformers** | 推理/微调 | 专注句子嵌入，语义相似度计算 |
| **PyTorch** | 训练 | 自定义 MLM 训练 |
| **ONNX Runtime** | 推理 | 高效部署 BERT 类模型 |

**开源实现参考**：
- **BERT 官方**：[google-research/bert](https://github.com/google-research/bert)
- **RoBERTa**：HuggingFace `roberta-base`、`roberta-large`
- **DeBERTa**：[microsoft/DeBERTa](https://github.com/microsoft/DeBERTa)

**典型训练代码结构**（MLM 伪代码）：
```python
# 掩码语言模型训练核心
for batch in dataloader:
    input_ids = batch["input_ids"]
    
    # 随机遮盖 15% 的 token
    masked_ids, mask_positions, labels = random_mask(input_ids, mask_prob=0.15)
    
    logits = model(masked_ids)                          # 前向传播
    loss = cross_entropy(logits[mask_positions], labels) # 只计算被遮盖位置的损失
    loss.backward()
    optimizer.step()
```

---

### 1.3 编码器-解码器模型（Encoder-Decoder）

**核心思想**：
- **编码器**：双向理解输入（如一段英文）
- **解码器**：自回归生成输出（如对应的中文翻译）

**训练目标**：
- 编码器用双向注意力编码输入
- 解码器根据编码结果 + 已生成的 token 预测下一个 token

**特点**：
- 输入和输出可以是不同长度、不同"形态"
- 天然适合序列到序列（Seq2Seq）任务

**架构**：完整的 Encoder-Decoder Transformer

**代表模型**：
| 模型 | 开发者 | 说明 |
|-----|-------|------|
| T5 | Google | "Text-to-Text"统一框架 |
| BART | Meta | 去噪自编码器 |
| Flan-T5 | Google | 指令微调版 T5 |
| mT5 | Google | 多语言版 T5 |

> **适用场景**：机器翻译、文本摘要、问答（有明确输入输出边界的任务）。

**主流框架与实现**：

| 框架/库 | 类型 | 说明 |
|--------|------|------|
| **HuggingFace Transformers** | 训练/推理 | 提供 `T5ForConditionalGeneration`、`BartForConditionalGeneration` |
| **Fairseq** | 训练 | Meta 出品，机器翻译领域常用 |
| **OpenNMT** | 训练/推理 | 专注神经机器翻译 |
| **MarianMT** | 推理 | HuggingFace 集成的多语言翻译模型 |

**开源实现参考**：
- **T5**：[google-research/t5x](https://github.com/google-research/t5x)
- **BART**：HuggingFace `facebook/bart-large`
- **mBART**：多语言版 BART

**典型训练代码结构**（Seq2Seq 伪代码）：
```python
# Encoder-Decoder 训练核心
for batch in dataloader:
    src_ids = batch["source_ids"]      # 源序列（如英文）
    tgt_ids = batch["target_ids"]      # 目标序列（如中文）
    
    # 编码器处理源序列
    encoder_output = encoder(src_ids)
    
    # 解码器根据编码结果 + 已生成 token 预测下一个
    decoder_input = tgt_ids[:, :-1]
    labels = tgt_ids[:, 1:]
    
    logits = decoder(decoder_input, encoder_output)
    loss = cross_entropy(logits, labels)
    loss.backward()
    optimizer.step()
```

---

### 1.4 前缀语言模型（Prefix LM）

**核心思想**：前缀部分使用双向注意力，后续部分使用自回归生成。

**特点**：
- 试图结合 AE（理解）和 AR（生成）的优点
- 前缀可以充分理解上下文，后续可以流畅生成

**代表模型**：
| 模型 | 开发者 | 说明 |
|-----|-------|------|
| GLM | 清华 | 自回归空白填充 |
| UniLM | Microsoft | 统一语言模型 |
| PaLM（部分机制） | Google | 混合注意力 |

**主流框架与实现**：

| 框架/库 | 类型 | 说明 |
|--------|------|------|
| **HuggingFace Transformers** | 训练/推理 | 支持 GLM 系列模型 |
| **ChatGLM 官方库** | 训练/推理 | 清华 GLM 系列专用 |

**开源实现参考**：
- **GLM-130B / ChatGLM**：[THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)
- **GLM-4**：[THUDM/GLM-4](https://github.com/THUDM/GLM-4)

**核心实现思路**：
```python
# Prefix LM 的注意力掩码设计
# 前缀部分：双向注意力（可以看到前缀内所有 token）
# 生成部分：单向注意力（只能看到之前的 token）

def create_prefix_lm_mask(prefix_len, total_len):
    mask = torch.zeros(total_len, total_len)
    # 前缀部分：双向
    mask[:prefix_len, :prefix_len] = 1
    # 生成部分：单向 + 可看前缀
    for i in range(prefix_len, total_len):
        mask[i, :i+1] = 1
    return mask
```

---

### 1.5 四种建模目标对比

| 类型 | 注意力方向 | 训练目标 | 擅长任务 | 典型架构 |
|-----|-----------|---------|---------|---------|
| 自回归（AR） | 单向（左→右） | 预测下一个 token | 生成、对话 | Decoder-only |
| 自编码（AE） | 双向 | 预测被遮盖 token | 理解、分类 | Encoder-only |
| 编码器-解码器 | 编码双向 + 解码单向 | Seq2Seq | 翻译、摘要 | Encoder-Decoder |
| 前缀 LM | 前缀双向 + 后续单向 | 混合 | 理解+生成 | 混合 |

### 1.6 各建模类型框架汇总

| 建模类型 | 主流训练框架 | 主流推理框架 | 代表开源实现 |
|---------|-------------|-------------|-------------|
| 自回归（AR） | PyTorch + DeepSpeed/Megatron | vLLM、TGI、TensorRT-LLM | LLaMA、Qwen、GPT-NeoX |
| 自编码（AE） | HuggingFace Transformers | ONNX Runtime、Triton | BERT、RoBERTa、DeBERTa |
| Encoder-Decoder | Fairseq、HuggingFace | HuggingFace、OpenNMT | T5、BART、mT5 |
| 前缀 LM | ChatGLM 官方、HuggingFace | vLLM（部分支持）、官方库 | GLM-4、ChatGLM |

---

## 二、按模态分类

模态指模型能处理的数据类型。

### 2.1 纯文本大模型（Text LLM）

只处理文本输入，输出文本。

| 模型 | 说明 |
|-----|------|
| GPT-4（文本版） | OpenAI 旗舰文本模型 |
| LLaMA 3 | Meta 开源系列 |
| Qwen 2.5 | 阿里开源系列 |
| Mistral / Mixtral | Mistral AI |

**主流框架**：
- **训练**：PyTorch + DeepSpeed / Megatron-LM / Colossal-AI
- **推理**：vLLM、TGI、TensorRT-LLM、llama.cpp、Ollama

---

### 2.2 多模态大模型（Multimodal LLM, MLLM）

能同时处理多种模态（文本、图像、音频、视频等）。

**典型架构**：
```
图像/音频/视频 → 编码器（如 ViT、Whisper）→ 投影层 → LLM → 文本输出
```

**代表模型**：

| 模型 | 支持模态 | 开发者 |
|-----|---------|-------|
| GPT-4o | 文本 + 图像 + 音频 | OpenAI |
| Gemini 1.5 | 文本 + 图像 + 音频 + 视频 | Google |
| Claude 3.5 | 文本 + 图像 | Anthropic |
| Qwen-VL / Qwen2-VL | 文本 + 图像 | 阿里 |
| LLaVA | 文本 + 图像 | 开源社区 |
| InternVL | 文本 + 图像 | 上海 AI Lab |

**主流框架与实现**：

| 框架/库 | 说明 |
|--------|------|
| **LLaVA** | 最流行的开源多模态方案，ViT + LLM |
| **InternVL** | 上海 AI Lab，性能强劲 |
| **Qwen-VL** | 阿里多模态系列 |
| **XComposer** | 上海 AI Lab，图文创作 |

**开源实现参考**：
- **LLaVA**：[haotian-liu/LLaVA](https://github.com/haotian-liu/LLaVA)
- **InternVL**：[OpenGVLab/InternVL](https://github.com/OpenGVLab/InternVL)
- **Qwen-VL**：[QwenLM/Qwen-VL](https://github.com/QwenLM/Qwen-VL)

**典型架构实现**：
```python
# 多模态 LLM 典型架构
class MultimodalLLM:
    def __init__(self):
        self.vision_encoder = ViT()           # 视觉编码器（如 CLIP-ViT）
        self.projector = nn.Linear(...)       # 投影层：对齐视觉与文本空间
        self.llm = LLaMA()                    # 语言模型骨干
    
    def forward(self, image, text):
        # 1. 图像编码
        image_features = self.vision_encoder(image)
        # 2. 投影到 LLM 空间
        image_tokens = self.projector(image_features)
        # 3. 拼接图像 token 和文本 token
        input_embeds = concat(image_tokens, text_embeds)
        # 4. LLM 生成
        output = self.llm(input_embeds)
        return output
```

---

### 2.3 视觉语言模型（Vision-Language Model, VLM）

专注于图像与文本的联合理解，通常用于：
- 图文匹配/检索
- 图像描述（Image Captioning）
- 视觉问答（VQA）

| 模型 | 特点 |
|-----|------|
| CLIP | 对比学习，图文对齐 |
| BLIP-2 | Q-Former 桥接视觉与语言 |
| SigLIP | CLIP 改进版 |

**开源实现**：
- **CLIP**：[openai/CLIP](https://github.com/openai/CLIP)
- **BLIP-2**：[salesforce/LAVIS](https://github.com/salesforce/LAVIS)
- **OpenCLIP**：[mlfoundations/open_clip](https://github.com/mlfoundations/open_clip)

---

### 2.4 语音语言模型

处理语音输入或输出。

| 模型 | 能力 |
|-----|------|
| Whisper | 语音识别（ASR） |
| Qwen-Audio | 语音理解 + 文本生成 |
| GPT-4o | 原生语音对话 |

**开源实现**：
- **Whisper**：[openai/whisper](https://github.com/openai/whisper)
- **Qwen-Audio**：[QwenLM/Qwen-Audio](https://github.com/QwenLM/Qwen-Audio)

---

### 2.5 视频理解模型

处理视频序列，理解时序信息。

| 模型 | 说明 |
|-----|------|
| Gemini 1.5 Pro | 支持长视频（数小时） |
| VideoLLaMA | 开源视频理解 |
| GPT-4o | 支持视频输入 |

**开源实现**：
- **Video-LLaVA**：[PKU-YuanGroup/Video-LLaVA](https://github.com/PKU-YuanGroup/Video-LLaVA)
- **VideoLLaMA**：[DAMO-NLP-SG/Video-LLaMA](https://github.com/DAMO-NLP-SG/Video-LLaMA)

---

## 三、按模型结构分类

### 3.1 稠密模型（Dense Model）

**特点**：每次推理时，所有参数都参与计算。

**优点**：
- 结构简单，训练稳定
- 推理逻辑清晰

**缺点**：
- 参数量 = 计算量，扩展成本高

**代表**：LLaMA、GPT-4（推测）、Qwen、Mistral

**主流框架**：
- **训练**：PyTorch + DeepSpeed（ZeRO Stage 2/3）、Megatron-LM、FSDP
- **推理**：vLLM、TGI、TensorRT-LLM

---

### 3.2 混合专家模型（Mixture of Experts, MoE）

**特点**：模型包含多个"专家"子网络，每次推理只激活其中一部分。

**架构示意**：
```
输入 → 路由器（Router）→ 选择 Top-K 专家 → 专家计算 → 加权合并 → 输出
```

**优点**：
- 参数量大但计算量可控（如 671B 参数只激活 37B）
- 可以扩展到更大规模

**缺点**：
- 训练更复杂（负载均衡、专家坍缩问题）
- 显存占用仍与总参数量相关

**代表模型**：
| 模型 | 总参数 | 激活参数 | 专家数 |
|-----|-------|---------|-------|
| Mixtral 8x7B | 47B | ~13B | 8 |
| DeepSeek-V3 | 671B | 37B | 256 |
| Grok-1 | 314B | ~86B | 8 |

**主流框架与实现**：

| 框架/库 | 说明 |
|--------|------|
| **Megatron-LM + MoE 扩展** | NVIDIA 官方 MoE 支持 |
| **DeepSpeed-MoE** | 微软 MoE 训练优化 |
| **Fairseq MoE** | Meta 的 MoE 实现 |
| **vLLM** | 推理时支持 MoE 模型（如 Mixtral） |
| **SGLang** | 高效 MoE 推理 |

**开源实现参考**：
- **Mixtral**：[mistralai/mistral-src](https://github.com/mistralai/mistral-src)
- **DeepSeek-MoE**：[deepseek-ai/DeepSeek-MoE](https://github.com/deepseek-ai/DeepSeek-MoE)
- **OpenMoE**：[XueFuzhao/OpenMoE](https://github.com/XueFuzhao/OpenMoE)

**MoE 核心实现**（伪代码）：
```python
class MoELayer(nn.Module):
    def __init__(self, num_experts, top_k):
        self.experts = nn.ModuleList([FFN() for _ in range(num_experts)])
        self.router = nn.Linear(hidden_size, num_experts)  # 路由器
        self.top_k = top_k
    
    def forward(self, x):
        # 1. 计算路由分数
        router_logits = self.router(x)
        router_probs = softmax(router_logits)
        
        # 2. 选择 Top-K 专家
        top_k_probs, top_k_indices = topk(router_probs, self.top_k)
        
        # 3. 分发到专家并计算
        output = 0
        for i, expert_idx in enumerate(top_k_indices):
            expert_output = self.experts[expert_idx](x)
            output += top_k_probs[i] * expert_output
        
        return output
```

---

### 3.3 Dense vs MoE 对比

| 维度 | Dense | MoE |
|-----|-------|-----|
| 参数利用率 | 100% | 10-30%（每次推理） |
| 训练复杂度 | 较低 | 较高（路由、负载均衡） |
| 推理计算量 | 与参数量成正比 | 可控（只激活部分专家） |
| 显存占用 | 与参数量成正比 | 仍需加载全部参数 |
| 扩展性 | 受限 | 更易扩展到超大规模 |

---

## 四、按训练阶段分类

一个完整的 LLM 通常经历以下阶段：

### 4.1 预训练（Pre-training）

**目标**：在海量无标注文本上学习语言的通用模式。

**数据**：网页、书籍、代码、论文等（TB 级别）

**产出**：Base 模型（如 `LLaMA-3-70B-Base`）

**特点**：
- 计算量最大（数千/数万 GPU 天）
- 模型学会语言知识，但不会遵循指令

**主流框架**：
- **Megatron-LM**：超大规模预训练首选
- **DeepSpeed**：ZeRO 显存优化
- **Colossal-AI**：多种并行策略
- **GPT-NeoX**：EleutherAI 开源预训练框架

---

### 4.2 监督微调（Supervised Fine-Tuning, SFT）

**目标**：让模型学会遵循指令、按格式回答。

**数据**：人工标注的"指令-回答"对（万~百万条）

**产出**：Instruct/Chat 模型（如 `LLaMA-3-70B-Instruct`）

**特点**：
- 计算量相对较小
- 模型开始"听话"，但可能不够"好用"

**主流框架**：

| 框架 | 特点 |
|-----|------|
| **LLaMA-Factory** | 一站式微调框架，支持多种方法 |
| **Axolotl** | 配置驱动，易于使用 |
| **FastChat** | LMSYS 出品，支持多模型 |
| **PEFT** | HuggingFace 参数高效微调库（LoRA、QLoRA 等） |
| **Unsloth** | 2x 加速，显存优化 |

**开源实现参考**：
- **LLaMA-Factory**：[hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)
- **Axolotl**：[OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)

**常见微调方法**：

| 方法 | 原理 | 显存需求 |
|-----|------|---------|
| 全参数微调（Full FT） | 更新所有参数 | 高 |
| LoRA | 低秩分解，只训练小矩阵 | 低 |
| QLoRA | 量化 + LoRA | 更低 |
| Prefix Tuning | 只训练前缀向量 | 低 |
| Adapter | 插入小型适配层 | 低 |

---

### 4.3 人类偏好对齐（Alignment）

**目标**：让模型输出更符合人类偏好（有用、诚实、无害）。

**常见方法**：

| 方法 | 原理 |
|-----|------|
| **RLHF** | 训练奖励模型 → 用 PPO 强化学习优化 |
| **DPO** | 直接用偏好数据优化，无需奖励模型 |
| **RLAIF** | 用 AI 生成偏好标签替代人工 |

**主流框架**：

| 框架 | 支持方法 | 说明 |
|-----|---------|------|
| **TRL** | RLHF、DPO、PPO | HuggingFace 官方，最主流 |
| **DeepSpeed-Chat** | RLHF | 微软出品，三阶段训练 |
| **OpenRLHF** | RLHF、DPO | 高效分布式 RLHF |
| **LLaMA-Factory** | DPO、ORPO | 集成多种对齐方法 |

**开源实现参考**：
- **TRL**：[huggingface/trl](https://github.com/huggingface/trl)
- **OpenRLHF**：[OpenRLHF/OpenRLHF](https://github.com/OpenRLHF/OpenRLHF)
- **DeepSpeed-Chat**：[microsoft/DeepSpeedExamples](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)

**产出**：对齐后的模型（更安全、更有帮助）

---

### 4.4 训练阶段流程图

```
┌─────────────────────────────────────────────────────────────────┐
│                        预训练（Pre-training）                    │
│  数据：海量无标注文本（TB级）                                      │
│  目标：学习语言模式                                               │
│  产出：Base 模型                                                 │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                    监督微调（SFT）                                │
│  数据：指令-回答对（万~百万条）                                    │
│  目标：学会遵循指令                                               │
│  产出：Instruct 模型                                             │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                    偏好对齐（RLHF/DPO）                           │
│  数据：人类偏好标注（好/坏回答对比）                                │
│  目标：输出更有用、更安全                                         │
│  产出：对齐后的模型                                               │
└─────────────────────────────────────────────────────────────────┘
```

---

## 五、按应用场景分类

| 类型 | 优化方向 | 代表模型 |
|-----|---------|---------|
| 通用对话 | 多轮对话、指令遵循 | ChatGPT、Claude、Qwen-Chat |
| 代码生成 | 编程语言理解与生成 | Codex、CodeLLaMA、DeepSeek-Coder |
| 数学推理 | 数学问题求解 | Qwen-Math、DeepSeek-Math |
| 长上下文 | 处理超长文档 | Claude 3（200K）、Gemini 1.5（1M+） |
| 工具调用 | Function Calling、Agent | GPT-4、Claude、Qwen |

**各场景主流框架**：

| 场景 | 主流框架/工具 |
|-----|--------------|
| 代码生成 | StarCoder、CodeLLaMA、DeepSeek-Coder |
| 数学推理 | Qwen-Math、DeepSeek-Math、Llemma |
| 长上下文 | YaRN、LongRoPE（位置编码扩展） |
| 工具调用 | LangChain、LlamaIndex、OpenAI Function Calling |
| Agent | AutoGPT、MetaGPT、AgentGPT |

---

## 六、总结：如何快速定位一个模型

当你看到一个新模型时，可以从以下维度快速理解它：

```
1. 建模目标：自回归 / 自编码 / Encoder-Decoder？
2. 模态：纯文本 / 多模态？
3. 结构：Dense / MoE？
4. 阶段：Base / Instruct / 对齐后？
5. 规模：参数量多少？激活参数多少（MoE）？
6. 场景：通用 / 代码 / 数学 / 长上下文？
```

**当前主流配置**：
- **自回归 + Decoder-only + Dense/MoE + 对齐后** = 现代对话 LLM 的标准形态

---

## 七、框架速查表

### 7.1 训练框架

| 框架 | 适用阶段 | 特点 | GitHub |
|-----|---------|------|--------|
| PyTorch | 全阶段 | 基础框架 | pytorch/pytorch |
| DeepSpeed | 预训练/SFT | ZeRO 显存优化 | microsoft/DeepSpeed |
| Megatron-LM | 预训练 | 超大规模并行 | NVIDIA/Megatron-LM |
| LLaMA-Factory | SFT/对齐 | 一站式微调 | hiyouga/LLaMA-Factory |
| TRL | 对齐 | RLHF/DPO | huggingface/trl |
| PEFT | SFT | LoRA/QLoRA | huggingface/peft |

### 7.2 推理框架

| 框架 | 特点 | 适用场景 | GitHub |
|-----|------|---------|--------|
| vLLM | PagedAttention，高吞吐 | 生产部署 | vllm-project/vllm |
| TGI | HF 生态集成 | 快速部署 | huggingface/text-generation-inference |
| TensorRT-LLM | 极致性能 | 低延迟场景 | NVIDIA/TensorRT-LLM |
| llama.cpp | CPU/边缘友好 | 本地部署 | ggerganov/llama.cpp |
| SGLang | 结构化生成 | 复杂输出 | sgl-project/sglang |
| Ollama | 一键运行 | 个人体验 | ollama/ollama |
